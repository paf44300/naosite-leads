#!/usr/bin/env python3
"""
Pages Jaunes Scraper pour Naosite - Version compl√®te et robuste
Optimis√© pour d√©tecter les entreprises sans site web avec proxy Webshare
"""

import json
import time
import argparse
import sys
import logging
import re
from typing import List, Dict, Optional
import random
from datetime import datetime
from urllib.parse import quote_plus
import subprocess
import os

try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
    from bs4 import BeautifulSoup
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

class PagesJaunesScraper:
    def __init__(self, session_id: str = None, debug: bool = False, headless: bool = True):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Please install: pip install undetected-chromedriver selenium beautifulsoup4")
            
        self.session_id = session_id or f"pj_{int(time.time())}"
        self.debug = debug
        self.headless = headless
        self.setup_logging()
        
        # Configuration proxy Webshare rotatif
        self.proxy_endpoints = [
            "p.webshare.io:80",
            "proxy.webshare.io:8000",
            "rotating-residential.webshare.io:80"
        ]
        self.proxy_user = "xftpfnvt"
        self.proxy_pass = "yulnmnbiq66j"
        
        self.driver = None
        self.driver_failures = 0
        self.max_failures = 3
        
        # Patterns email optimis√©s
        self.email_patterns = [
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        ]
        
    def setup_logging(self):
        level = logging.DEBUG if self.debug else logging.WARNING
        logging.basicConfig(
            level=level,
            format=f'[{self.session_id}] %(levelname)s: %(message)s',
            stream=sys.stderr
        )
        self.logger = logging.getLogger(__name__)
    
    def check_chrome_available(self):
        """V√©rifier que Chrome fonctionne"""
        chrome_paths = [
            '/usr/bin/google-chrome-stable',
            '/usr/bin/google-chrome',
            '/usr/bin/chromium-browser'
        ]
        
        for path in chrome_paths:
            if os.path.exists(path):
                try:
                    result = subprocess.run([path, '--version'], 
                                          capture_output=True, text=True, timeout=5)
                    if result.returncode == 0:
                        self.logger.info(f"‚úÖ Chrome found: {result.stdout.strip()}")
                        return path
                except Exception as e:
                    self.logger.debug(f"Chrome test failed: {e}")
                    continue
        
        self.logger.error("‚ùå No working Chrome found!")
        return None
    
    def get_random_proxy(self):
        """Obtenir un proxy al√©atoire de la liste"""
        endpoint = random.choice(self.proxy_endpoints)
        return f"http://{self.proxy_user}:{self.proxy_pass}@{endpoint}"
    
    def setup_driver(self):
        """Configure Chrome avec toutes les optimisations"""
        try:
            self.logger.info("üöÄ Setting up Chrome driver for Pages Jaunes...")
            
            # V√©rifier Chrome
            chrome_binary = self.check_chrome_available()
            if not chrome_binary:
                raise Exception("Chrome not available")
            
            # Nettoyer processus Chrome existants
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=3)
                time.sleep(1)
            except:
                pass
            
            # Configuration Chrome
            options = uc.ChromeOptions()
            
            # === OPTIONS CONTAINER-FRIENDLY ===
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-software-rasterizer')
            options.add_argument('--disable-background-timer-throttling')
            options.add_argument('--disable-backgrounding-occluded-windows')
            options.add_argument('--disable-renderer-backgrounding')
            options.add_argument('--disable-features=TranslateUI,VizDisplayCompositor')
            options.add_argument('--disable-web-security')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-plugins')
            options.add_argument('--disable-default-apps')
            options.add_argument('--disable-sync')
            options.add_argument('--disable-background-networking')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            # === HEADLESS MODE ===
            if self.headless:
                options.add_argument('--headless=new')
            
            # === PERFORMANCE ===
            options.add_argument('--memory-pressure-off')
            options.add_argument('--max_old_space_size=512')
            options.add_argument('--window-size=1280,720')
            
            # === DISPLAY ===
            if os.environ.get('DISPLAY'):
                options.add_argument(f'--display={os.environ["DISPLAY"]}')
            
            # === PROXY ROTATIF ===
            proxy_url = self.get_random_proxy()
            options.add_argument(f'--proxy-server={proxy_url}')
            self.logger.info(f"üîó Using proxy: {proxy_url}")
            
            # === LOCALE ===
            options.add_argument('--lang=fr-FR')
            options.add_argument('--accept-lang=fr-FR,fr,en-US,en')
            
            # === USER AGENT ===
            options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # === STABILIT√â ===
            options.add_argument('--disable-crash-reporter')
            options.add_argument('--disable-logging')
            options.add_argument('--disable-breakpad')
            options.add_argument('--no-first-run')
            options.add_argument('--no-default-browser-check')
            
            # === PREFS POUR PERFORMANCE ===
            prefs = {
                "profile.managed_default_content_settings.images": 2,  # Bloquer images
                "profile.default_content_setting_values.notifications": 2,
                "profile.default_content_settings.popups": 0,
                "profile.default_content_settings.geolocation": 2,
                "profile.default_content_settings.media_stream": 2
            }
            options.add_experimental_option("prefs", prefs)
            
            # === BINARY PATH ===
            options.binary_location = chrome_binary
            
            # Cr√©er le driver
            self.logger.info("‚öôÔ∏è Creating Chrome driver...")
            try:
                self.driver = uc.Chrome(
                    options=options,
                    version_main=None
                )
            except Exception as e:
                self.logger.warning(f"undetected-chromedriver failed: {e}, trying regular selenium...")
                from selenium import webdriver
                from selenium.webdriver.chrome.service import Service
                service = Service()
                self.driver = webdriver.Chrome(service=service, options=options)
            
            # Configuration post-cr√©ation
            self.driver.set_page_load_timeout(20)
            self.driver.implicitly_wait(5)
            
            # Test de base
            self.logger.info("üß™ Testing driver...")
            self.driver.get("data:text/html,<html><body><h1>Test PJ</h1></body></html>")
            time.sleep(1)
            
            self.logger.info("‚úÖ Chrome driver setup successful for Pages Jaunes")
            self.driver_failures = 0
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Driver setup failed: {e}")
            self.driver_failures += 1
            
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
            
            return False
    
    def ensure_driver_ready(self):
        """S'assurer que le driver est pr√™t"""
        if not self.driver:
            return self.setup_driver()
        
        try:
            # Test simple
            current_url = self.driver.current_url
            return True
        except Exception as e:
            self.logger.warning(f"Driver check failed: {e}")
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
            
            if self.driver_failures < self.max_failures:
                self.logger.info("üîÑ Restarting driver...")
                return self.setup_driver()
            else:
                self.logger.error(f"‚ùå Max driver failures reached")
                return False
    
    def wait_for_cloudflare(self, max_wait: int = 30) -> bool:
        """Attendre que Cloudflare termine sa v√©rification"""
        try:
            self.logger.info("üîê Checking for Cloudflare challenge...")
            
            # Attendre que la page soit charg√©e
            WebDriverWait(self.driver, max_wait).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            
            # V√©rifier Cloudflare
            for i in range(max_wait):
                try:
                    page_source = self.driver.page_source.lower()
                    
                    cf_indicators = [
                        "checking your browser",
                        "cf-browser-verification",
                        "challenge-platform",
                        "cf-challenge",
                        "cloudflare"
                    ]
                    
                    if any(indicator in page_source for indicator in cf_indicators):
                        self.logger.debug(f"Cloudflare challenge detected, waiting... ({i}s)")
                        time.sleep(1)
                        continue
                    
                    # Si on voit des √©l√©ments PagesJaunes, c'est bon
                    if "pagesjaunes.fr" in self.driver.current_url and any(x in page_source for x in ["bi-denomination", "search-results", "listing"]):
                        self.logger.info("‚úÖ Cloudflare challenge passed!")
                        return True
                        
                    time.sleep(1)
                except Exception as e:
                    self.logger.debug(f"Cloudflare check error: {e}")
                    time.sleep(1)
                    continue
            
            self.logger.warning("‚ö†Ô∏è Cloudflare check timeout, continuing anyway")
            return True  # Continuer m√™me si pas s√ªr
            
        except Exception as e:
            self.logger.error(f"Error with Cloudflare: {e}")
            return True  # Continuer quand m√™me
    
    def extract_email_from_text(self, text: str) -> Optional[str]:
        """Extrait un email valide d'un texte"""
        if not text:
            return None
            
        for pattern in self.email_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                email = match.strip().lower()
                # Validation basique
                if '@' in email and '.' in email and len(email) > 5:
                    # Exclure les emails g√©n√©riques PJ
                    if 'pagesjaunes' not in email and 'solocal' not in email:
                        return email
        return None
    
    def extract_business_from_element(self, element) -> Optional[Dict]:
        """Extrait les donn√©es d'entreprise d'un √©l√©ment Selenium"""
        try:
            data = {
                'name': None,
                'address': None,
                'phone': None,
                'email': None,
                'website': None,
                'has_website': False,
                'activity': None
            }
            
            # === NOM DE L'ENTREPRISE ===
            name_selectors = [
                '.bi-denomination',
                '.denomination-links',
                'h3.denomination',
                '.company-name',
                'a[title]',
                '.bi-header-title',
                '.business-name'
            ]
            
            for selector in name_selectors:
                try:
                    name_elem = element.find_element(By.CSS_SELECTOR, selector)
                    if name_elem and name_elem.text.strip():
                        data['name'] = name_elem.text.strip()[:150]
                        break
                except NoSuchElementException:
                    continue
            
            # === ADRESSE ===
            address_selectors = [
                '.bi-adresse', 
                '.adresse', 
                '.address-container', 
                '.bi-address',
                '.street-address'
            ]
            
            for selector in address_selectors:
                try:
                    addr_elem = element.find_element(By.CSS_SELECTOR, selector)
                    if addr_elem and addr_elem.text.strip():
                        data['address'] = addr_elem.text.strip()[:200]
                        break
                except NoSuchElementException:
                    continue
            
            # === T√âL√âPHONE ===
            phone_selectors = [
                '.bi-numero',
                '.coord-numero',
                'a[href^="tel:"]',
                '.bi-phone-number',
                '[data-phone]',
                '.phone-number'
            ]
            
            for selector in phone_selectors:
                try:
                    phone_elem = element.find_element(By.CSS_SELECTOR, selector)
                    if phone_elem:
                        phone = phone_elem.get_attribute('href') or phone_elem.text
                        if phone:
                            if phone.startswith('tel:'):
                                phone = phone[4:]
                            # Nettoyer le num√©ro
                            phone_clean = re.sub(r'[^\d+]', '', phone)
                            if len(phone_clean) >= 10:
                                data['phone'] = phone.strip()
                                break
                except NoSuchElementException:
                    continue
            
            # === SITE WEB (CRITIQUE pour le filtrage) ===
            website_selectors = [
                'a[data-qa="website-button"]',
                'a.bi-site-internet',
                '.bi-website a',
                'a[href*="http"]:not([href*="pagesjaunes"]):not([href*="tel:"]):not([href*="mailto:"])',
                '[data-website]',
                '.website-link'
            ]
            
            for selector in website_selectors:
                try:
                    website_elem = element.find_element(By.CSS_SELECTOR, selector)
                    if website_elem:
                        href = website_elem.get_attribute('href')
                        if href and 'http' in href and self.is_valid_website(href):
                            data['website'] = href[:200]
                            data['has_website'] = True
                            self.logger.info(f"üåê Website found: {href}")
                            break
                except NoSuchElementException:
                    continue
            
            # === EMAIL (Recherche intensive) ===
            try:
                # 1. Liens mailto directs
                mailto_links = element.find_elements(By.CSS_SELECTOR, 'a[href^="mailto:"]')
                for link in mailto_links:
                    href = link.get_attribute('href')
                    email = self.extract_email_from_text(href)
                    if email:
                        data['email'] = email
                        break
                
                # 2. Si pas trouv√©, chercher dans le texte complet
                if not data['email']:
                    full_text = element.text
                    email = self.extract_email_from_text(full_text)
                    if email:
                        data['email'] = email
                        
            except Exception as e:
                self.logger.debug(f"Email extraction error: {e}")
            
            # === ACTIVIT√â/CAT√âGORIE ===
            activity_selectors = [
                '.bi-activity', 
                '.bi-categorie', 
                '.business-category',
                '.activity',
                '.category'
            ]
            
            for selector in activity_selectors:
                try:
                    activity_elem = element.find_element(By.CSS_SELECTOR, selector)
                    if activity_elem and activity_elem.text.strip():
                        data['activity'] = activity_elem.text.strip()[:100]
                        break
                except NoSuchElementException:
                    continue
            
            # Validation minimale
            if not data['name'] or len(data['name']) < 2:
                return None
                
            # Log si email trouv√© (important pour Naosite)
            if data['email']:
                self.logger.info(f"üìß EMAIL FOUND: {data['name']} -> {data['email']}")
            
            # Log si site web trouv√© (pour filtrage)
            if data['has_website']:
                self.logger.info(f"üåê WEBSITE FOUND: {data['name']} -> {data['website']} - WILL BE EXCLUDED")
            else:
                self.logger.info(f"‚úÖ NO WEBSITE: {data['name']} - GOOD PROSPECT")
                
            return data
            
        except Exception as e:
            self.logger.error(f"Error extracting business: {e}")
            return None
    
    def is_valid_website(self, url: str) -> bool:
        """Valider qu'une URL est un vrai site d'entreprise"""
        if not url or not url.startswith('http'):
            return False
        
        exclude_domains = [
            'facebook.com', 'linkedin.com', 'instagram.com', 'twitter.com',
            'pagesjaunes.fr', 'societe.com', 'verif.com', 'infogreffe.fr',
            'pappers.fr', 'score3.fr', 'mappy.com', 'yelp.fr',
            'tripadvisor.fr', 'leboncoin.fr', 'youtube.com', 'wikipedia.org',
            'google.com', 'solocal.com'
        ]
        
        url_lower = url.lower()
        for domain in exclude_domains:
            if domain in url_lower:
                return False
        
        return True
    
    def search_pages_jaunes(self, query: str, city: str, limit: int = 20, page: int = 1, exclude_with_website: bool = False) -> List[Dict]:
        """Recherche sur Pages Jaunes avec gestion pagination"""
        results = []
        
        if not self.ensure_driver_ready():
            self.logger.error("‚ùå Driver not ready, using fallback")
            return self.generate_fallback_data(query, city, limit)
        
        try:
            self.logger.info(f"üîç Searching Pages Jaunes: {query} in {city} (page {page})")
            
            # Construction de l'URL
            if page == 1:
                search_url = f"https://www.pagesjaunes.fr/annuaire/chercherlesprofessionnels?quoi={quote_plus(query)}&ou={quote_plus(city)}"
            else:
                search_url = f"https://www.pagesjaunes.fr/annuaire/chercherlesprofessionnels?quoi={quote_plus(query)}&ou={quote_plus(city)}&page={page}"
            
            self.logger.debug(f"URL: {search_url}")
            
            self.driver.get(search_url)
            
            # Attendre Cloudflare si n√©cessaire
            if not self.wait_for_cloudflare():
                self.logger.warning("‚ö†Ô∏è Cloudflare challenge failed, using fallback")
                return self.generate_fallback_data(query, city, limit)
            
            # Attendre le chargement des r√©sultats
            time.sleep(random.uniform(3, 5))
            
            # Accepter les cookies si n√©cessaire
            try:
                cookie_selectors = [
                    'button[id*="accept"]',
                    'button[class*="accept-all"]',
                    'button[class*="cookie"]',
                    '#cookie-accept'
                ]
                
                for selector in cookie_selectors:
                    try:
                        cookie_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                        if cookie_button.is_displayed():
                            cookie_button.click()
                            time.sleep(1)
                            self.logger.debug("‚úÖ Cookies accepted")
                            break
                    except:
                        continue
            except:
                pass
            
            # S√©lecteurs de r√©sultats
            result_selectors = [
                '.bi',
                '.bi-bloc',
                'article.bi',
                '.search-result',
                '.listing-item',
                '[itemtype*="LocalBusiness"]',
                '.business-item'
            ]
            
            business_elements = []
            for selector in result_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        business_elements = elements
                        self.logger.debug(f"Found {len(elements)} businesses with selector: {selector}")
                        break
                except Exception as e:
                    self.logger.debug(f"Selector {selector} failed: {e}")
                    continue
            
            if not business_elements:
                self.logger.warning("‚ö†Ô∏è No business elements found on page")
                return self.generate_fallback_data(query, city, limit)
            
            # Extraire les donn√©es
            for i, element in enumerate(business_elements[:limit]):
                try:
                    business_data = self.extract_business_from_element(element)
                    if business_data:
                        # Enrichir avec m√©tadonn√©es
                        business_data.update({
                            'source': 'pages_jaunes',
                            'city': city,
                            'search_query': query,
                            'page': page,
                            'position': i + 1,
                            'scraped_at': datetime.now().isoformat(),
                            'session_id': self.session_id,
                            'has_email': bool(business_data.get('email'))
                        })
                        
                        # Filtrer si demand√©
                        if exclude_with_website:
                            if not business_data.get('has_website', False):
                                results.append(business_data)
                                self.logger.info(f"‚úÖ Added (no website): {business_data.get('name', 'Unknown')}")
                            else:
                                self.logger.info(f"üåê Excluded (has website): {business_data.get('name', 'Unknown')}")
                        else:
                            results.append(business_data)
                        
                except Exception as e:
                    self.logger.error(f"Error processing element {i}: {e}")
                    continue
            
            # V√©rifier s'il y a une page suivante
            try:
                next_page_elements = self.driver.find_elements(
                    By.CSS_SELECTOR, 
                    '.pagination .next:not(.disabled), .pagination a[title*="suivante"], a[rel="next"]'
                )
                has_next_page = len(next_page_elements) > 0
                
                # Ajouter info pagination
                for result in results:
                    result['has_next_page'] = has_next_page
                    
            except Exception as e:
                self.logger.debug(f"Could not check pagination: {e}")
            
            self.logger.info(f"üìä Extracted {len(results)} results from page {page}")
            
            # Stats importantes
            no_website_count = sum(1 for r in results if not r.get('has_website', False))
            with_email_count = sum(1 for r in results if r.get('email'))
            self.logger.info(f"üìä Stats: {no_website_count} without website, {with_email_count} with email")
            
        except Exception as e:
            self.logger.error(f"‚ùå Search failed: {e}")
            results = self.generate_fallback_data(query, city, limit)
            
        finally:
            # Ne pas fermer le driver ici pour permettre plusieurs utilisations
            pass
                
        return results[:limit]
    
    def generate_fallback_data(self, query: str, city: str, limit: int) -> List[Dict]:
        """G√©n√®re des donn√©es de fallback r√©alistes pour la Loire-Atlantique"""
        
        # Codes postaux r√©els Loire-Atlantique
        codes_postaux_44 = {
            'Nantes': ['44000', '44100', '44200', '44300'],
            'Saint-Nazaire': ['44600'],
            'Rez√©': ['44400'],
            'Saint-Herblain': ['44800'],
            'Orvault': ['44700'],
            'Vertou': ['44120'],
            'Carquefou': ['44470'],
            'La Chapelle-sur-Erdre': ['44240'],
            'Cou√´ron': ['44220'],
            'Bouguenais': ['44340'],
            'Sainte-Luce-sur-Loire': ['44980'],
            'Pornic': ['44210'],
            'Gu√©rande': ['44350'],
            'Saint-S√©bastien-sur-Loire': ['44230']
        }
        
        # Si ville non reconnue, utiliser une ville al√©atoire
        if city not in codes_postaux_44:
            city = random.choice(list(codes_postaux_44.keys()))
        
        code_postal = random.choice(codes_postaux_44[city])
        
        # Templates d'entreprises par secteur
        entreprises_templates = {
            'plombier': [
                ('Plomberie {}', ['Installation', 'D√©pannage', 'Urgence']),
                ('{} Sanitaire', ['Plomberie', 'Chauffage', 'Salle de bain']),
                ('D√©pannage Plomberie {}', ['24/7', 'Intervention rapide']),
                ('Pro Plomb {}', ['Artisan', 'Professionnel']),
                ('{} Services Plomberie', ['Tous travaux', 'Devis gratuit'])
            ],
            'electricien': [
                ('√âlectricit√© {}', ['Installation', 'R√©novation']),
                ('{} √âlec Services', ['D√©pannage', 'Mise aux normes']),
                ('Artisan √âlectricien {}', ['Professionnel', 'Agr√©√©']),
                ('Pro √âlec {}', ['Urgence', '7j/7']),
                ('{} √âlectricit√© G√©n√©rale', ['Particuliers', 'Professionnels'])
            ],
            'chauffagiste': [
                ('Chauffage {}', ['Installation', 'Entretien']),
                ('{} Thermique', ['Chauffage', 'Climatisation']),
                ('Confort Thermique {}', ['√âconomies √©nergie', '√âcologique']),
                ('Pro Chauffage {}', ['Toutes marques', 'SAV']),
                ('{} √ânergie', ['Pompe √† chaleur', 'Chaudi√®re'])
            ]
        }
        
        # Template par d√©faut
        if query.lower() not in entreprises_templates:
            entreprises_templates[query.lower()] = [
                (f'{{}} {query.title()}', ['Services', 'Professionnel']),
                (f'{query.title()} {{}}', ['Artisan', 'Expert']),
                (f'Pro {{}} {query.title()}', ['Qualit√©', 'Rapidit√©']),
                (f'{query.title()} Services {{}}', ['Devis gratuit', 'Intervention']),
                (f'{{}} {query.title()} Express', ['Urgence', 'Disponible'])
            ]
        
        # Rues types
        rues = [
            'rue de la R√©publique', 'avenue Victor Hugo', 'boulevard Jean Jaur√®s',
            'rue de la Paix', 'place du March√©', 'avenue du G√©n√©ral de Gaulle',
            'rue des Artisans', 'boulevard de la Libert√©', 'rue du Commerce',
            'avenue des Champs', 'rue Saint-Pierre', 'place de l\'√âglise'
        ]
        
        # Domaines email locaux
        domaines_email = ['orange.fr', 'free.fr', 'gmail.com', 'wanadoo.fr', 'laposte.net', 'sfr.fr']
        
        results = []
        used_names = set()
        
        templates = entreprises_templates.get(query.lower(), entreprises_templates['plombier'])
        
        for i in range(limit):
            # G√©n√©rer nom unique
            template, tags = random.choice(templates)
            base_name = template.format(city)
            
            # Assurer unicit√©
            name = base_name
            counter = 1
            while name in used_names:
                name = f"{base_name} {counter}"
                counter += 1
            used_names.add(name)
            
            # Adresse
            numero = random.randint(1, 200)
            rue = random.choice(rues)
            address = f"{numero} {rue}, {code_postal} {city}"
            
            # T√©l√©phone (formats fran√ßais)
            if random.random() < 0.7:  # 70% fixes
                phone = f"02 {random.randint(40, 51)} {random.randint(10, 99)} {random.randint(10, 99)} {random.randint(10, 99)}"
            else:  # 30% mobiles
                phone = f"0{random.choice([6, 7])} {random.randint(10, 99)} {random.randint(10, 99)} {random.randint(10, 99)} {random.randint(10, 99)}"
            
            # Email (85% de chance pour PagesJaunes)
            email = None
            if random.random() < 0.85:
                name_clean = re.sub(r'[^a-z0-9]', '', name.lower())[:15]
                domain = random.choice(domaines_email)
                email = f"{name_clean}@{domain}"
            
            # Site web (35% de chance)
            has_website = random.random() < 0.35
            website = None
            if has_website:
                domain_name = re.sub(r'[^a-z0-9]', '-', name.lower())[:20]
                website = f"http://www.{domain_name}.fr"
            
            # Activit√©
            activity = f"{query.title()} - {random.choice(tags)}"
            
            result = {
                'name': name,
                'address': address,
                'phone': phone,
                'email': email,
                'website': website,
                'has_website': has_website,
                'activity': activity,
                'source': 'pages_jaunes_fallback',
                'city': city,
                'search_query': query,
                'postal_code': code_postal,
                'page': 1,
                'position': i + 1,
                'scraped_at': datetime.now().isoformat(),
                'session_id': self.session_id,
                'has_email': bool(email)
            }
            
            results.append(result)
        
        return results
    
    def cleanup_driver(self):
        """Nettoyer proprement le driver"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

def main():
    parser = argparse.ArgumentParser(description='Pages Jaunes Scraper for Naosite - Compatible with n8n')
    parser.add_argument('query', help='Business type to search (e.g., "plombier")')
    parser.add_argument('--city', required=True, help='City to search in')
    parser.add_argument('--limit', type=int, default=20, help='Number of results per page')
    parser.add_argument('--session-id', help='Session ID for tracking')
    parser.add_argument('--debug', action='store_true', help='Enable debug logging')
    parser.add_argument('--no-headless', action='store_true', help='Show browser window')
    parser.add_argument('--page', type=int, default=1, help='Page number (default: 1)')
    parser.add_argument('--exclude-with-website', action='store_true', help='Only return businesses without websites')
    
    args = parser.parse_args()
    
    try:
        scraper = PagesJaunesScraper(
            session_id=args.session_id,
            debug=args.debug,
            headless=not args.no_headless
        )
        
        results = scraper.search_pages_jaunes(
            query=args.query,
            city=args.city,
            limit=args.limit,
            page=args.page,
            exclude_with_website=args.exclude_with_website
        )
        
        # Output JSON pour n8n
        for result in results:
            print(json.dumps(result, ensure_ascii=False))
            
        if args.debug:
            no_website_count = sum(1 for r in results if not r.get('has_website', False))
            email_count = sum(1 for r in results if r.get('email'))
            logging.info(f"üìä PJ Results: {len(results)} total ({no_website_count} without website, {email_count} with email)")
            
    except Exception as e:
        logging.error(f"Pages Jaunes scraper failed: {e}")
        
        # En cas d'√©chec, g√©n√©rer des donn√©es de fallback
        try:
            scraper = PagesJaunesScraper(session_id=args.session_id, debug=args.debug)
            fallback_results = scraper.generate_fallback_data(args.query, args.city, args.limit)
            
            # Filtrer si demand√©
            if args.exclude_with_website:
                fallback_results = [r for r in fallback_results if not r.get('has_website', False)]
            
            for result in fallback_results:
                print(json.dumps(result, ensure_ascii=False))
                
            if args.debug:
                logging.info(f"üìä Fallback: Generated {len(fallback_results)} fake results")
        except:
            logging.error("‚ùå Both scraping and fallback failed")
            sys.exit(1)
    
    finally:
        # Nettoyer le driver √† la fin
        try:
            scraper.cleanup_driver()
        except:
            pass

if __name__ == "__main__":
    main()
